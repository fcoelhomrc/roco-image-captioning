{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:03.324391Z",
     "start_time": "2025-01-04T15:14:03.321678Z"
    }
   },
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:05.808456Z",
     "start_time": "2025-01-04T15:14:03.756627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", attn_implementation=\"sdpa\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "id": "a6fe6dfeba0ea9b6",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:06.703013Z",
     "start_time": "2025-01-04T15:14:05.816295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "images = [image, image, image, image, image]\n"
   ],
   "id": "856d3789e6de1d44",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:06.840967Z",
     "start_time": "2025-01-04T15:14:06.812463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a remote controller\", \"a photo of two cute cats\"]\n",
    "inputs = processor(text=text, images=images, return_tensors=\"pt\", padding=True)"
   ],
   "id": "aa0987b7ded08b4b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:06.955860Z",
     "start_time": "2025-01-04T15:14:06.952961Z"
    }
   },
   "cell_type": "code",
   "source": "inputs[\"pixel_values\"].shape, inputs[\"input_ids\"].shape, inputs[\"attention_mask\"].shape",
   "id": "7e90d505db9e4916",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 224, 224]), torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:07.242611Z",
     "start_time": "2025-01-04T15:14:07.067506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
   ],
   "id": "95c3bbb783a19258",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:07.359208Z",
     "start_time": "2025-01-04T15:14:07.356720Z"
    }
   },
   "cell_type": "code",
   "source": "logits_per_image.shape, probs.shape",
   "id": "efee5fa1af7a542c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([5, 4]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:07.460758Z",
     "start_time": "2025-01-04T15:14:07.458944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ],
   "id": "b06846cdd972bce9",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:07.643268Z",
     "start_time": "2025-01-04T15:14:07.564606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.imshow(100*probs.detach().numpy(), cmap=\"seismic\")\n",
    "plt.colorbar()"
   ],
   "id": "7f2bb361883dccf8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x77c980870b90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAGdCAYAAAA1yoVoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdbUlEQVR4nO3df2xV9R3/8ddt9d520Hu1uLY0tNpMIiKCCogVY5x2EqIEItlmwrKKZi6uZdZ+M6WJwHRq0WWKSi1oHGpih3NJYZqvML91tDFSwCILzImakXkz1qLJ6IUuvYV77/cP9G43gN7b++nnnvvp85GchHvuuee8e0N48X6fH/UlEomEAADIUkGuCwAAuIFAAQAYQaAAAIwgUAAARhAoAAAjCBQAgBEECgDACAIFAGDEObYPGI/HdfjwYZWUlMjn89k+PACcUSKR0LFjx1RZWamCgrH7v/bw8LBGRkaM7Mvv96uoqMjIvkywHiiHDx9WVVWV7cMCQFrC4bCmTJkyJvseHh7Wt4uLddzQ/ioqKnTo0CHPhIr1QCkpKfnyTz+Q5Ld9+Dzzea4LyBOFuS4gL6zU/811CZ4WlfSU/vffKPNGRkZ0XNL/kRTIcl9RSb/p79fIyMj4DZT/jrn8IlC+ybm5LiBPECjpyPYfsPHCxig+IMkbEWCW9UABgPGuQNlfEeXFK6oIFACwjEABABjhaqB4sSYAQB6iQwEAy1ztUAgUALDM1UDxYk0AgDxEhwIAlrnaoRAoAGCZq4HixZoAAHmIDgUALPMp+//Ne/FZ7QQKAFjmU/aB4MVAYeQFADCCDgUALCtU9s/I9uIztgkUALDM1au8CBQAsMzVQPFiTQCAPESHAgCWudqhECgAYJmrgeLFmgAAeYgOBQAsc7VDIVAAwDJXA8WLNQEA8hAdCgBY5uqzvAgUALCsQNk/OsWL4yUv1gQAyEN0KABgmasn5QkUALCMQAEAGOFqoIyqpra2Nl100UUqKirSvHnztHv3btN1AQDyTMaB8tprr6m5uVlr1qzR3r17NWvWLC1YsEBHjhwZi/oAwDkFhhavybimJ598Uj/5yU+0fPlyTZ8+XRs2bNC3vvUt/fa3vx2L+gDAOQSKpJGREfX19amuru6/OygoUF1dnXbu3Gm8OABA/sjopPwXX3yhWCym8vLylPXl5eX66KOPzviZaDSqaDSafB2JREZRJgC4g5Pyo9Ta2qpQKJRcqqqqxvqQAOBpPkOL12QUKBdccIEKCws1MDCQsn5gYEAVFRVn/ExLS4sGBweTSzgcHn21AADPyihQ/H6/Zs+era6uruS6eDyurq4u1dbWnvEzgUBAwWAwZQGA8azQ0OI1Gd/Y2NzcrPr6es2ZM0dXX3211q1bp6GhIS1fvnws6gMA5/iU/fkGL468Mg6UH/7wh/r888+1evVq9ff364orrtC2bdtOO1EPABhfRvXolcbGRjU2NpquBQDGBVev8uJZXgBgGYECADDC1UDxYk0AgDxEhwIAlrnaoRAoAGCZq4HixZoAAHmIDgUALDPxLC4nbmwEAGTHxKNTvPjoFUZeAAAj6FAAwDJXT8oTKABgmasPh/RiyAEA8hAdCgBYxsgLAGAEgQIAMMLVQPFiTQAAg2KxmFatWqWamhoVFxfrO9/5jn71q18pkUgkt0kkElq9erUmT56s4uJi1dXV6ZNPPsnoOAQKAFhWYGhJ1+OPP6729natX79ef/vb3/T444/riSee0LPPPpvc5oknntAzzzyjDRs2aNeuXZowYYIWLFig4eHhtI/DyAsALLP96JX33ntPixcv1i233CJJuuiii/S73/1Ou3fvlnSqO1m3bp0efPBBLV68WJL0yiuvqLy8XFu2bNHtt9+e1nHoUAAgj0UikZQlGo2ets21116rrq4uffzxx5Kkv/zlL3r33Xe1cOFCSdKhQ4fU39+vurq65GdCoZDmzZunnTt3pl0LHQoAWGbyWV5VVVUp69esWaNf/vKXKetWrlypSCSiadOmqbCwULFYTI8++qiWLVsmServ75cklZeXp3yuvLw8+V46CBQAsMzkVV7hcFjBYDC5PhAInLbt73//e7366qvq6OjQZZddpn379qmpqUmVlZWqr6/PspL/IlAAII8Fg8GUQDmTX/ziF1q5cmXyXMjll1+uf/zjH2ptbVV9fb0qKiokSQMDA5o8eXLycwMDA7riiivSroVzKABgme2rvP7zn/+ooCD1E4WFhYrH45KkmpoaVVRUqKurK/l+JBLRrl27VFtbm/Zx6FAAwDLbD4dctGiRHn30UVVXV+uyyy7TBx98oCeffFJ33nnnqX35fGpqatIjjzyiqVOnqqamRqtWrVJlZaWWLFmS9nEIFABw3LPPPqtVq1bpZz/7mY4cOaLKykr99Kc/1erVq5Pb3H///RoaGtLdd9+to0eP6rrrrtO2bdtUVFSU9nF8if+9VdKCSCSiUCgk6UeS/DYPnYeO5LqAPOHF313nPWu0NdcleFpU0lpJg4OD33hOYrS++vfvL5JKstzXMUmzNLb1ZooOBQAsc/VZXgQKAFjmaqB4sSYAQB6iQwEAy2w/y8sWAgUALDP56BUvYeQFADCCDgUALHP1pDyBAgCWuRooXqwJAJCH6FAAwDJXOxQCBQAss/1wSFu8GHIAgDxEhwIAljHyAgAYQaAAAIxwNVC8WBMAIA/RoQCAZa52KAQKAFjmaqB4sSYAQB6iQwEAy1ztUAgUALDM1UDxYk0AgDxEhwIAlrnaoRAoAGCZT5LPl93jHX2JhJliDPJiyAEA8hAdCgDYds45UpYdihIJ6eRJM/UYQqAAgG0ECgDACFOB4jGcQwEAGEGHAgC2OdqhECgAYFthoVSQ5YAoHjdTi0GMvAAARtChAIBt55zjZIdCoACAbY4GCiMvAIARdCgAYJujHQqBAgC2FRaeWrIRi5mpxaCMI7Knp0eLFi1SZWWlfD6ftmzZMgZlAQDyTcaBMjQ0pFmzZqmtrW0s6gEA951zjpnFYzKuaOHChVq4cOFY1AIA48M552Q/8sr2Tvsx4L2IAwDXESijE41GFY1Gk68jkchYHxIAkANjfh9Ka2urQqFQcqmqqhrrQwKAtzl6DmXMA6WlpUWDg4PJJRwOj/UhAcDbCguzD5NsR2ZjYMwjLhAIKBAIjPVhAAA5lnGgHD9+XJ9++mny9aFDh7Rv3z6VlpaqurraaHEA4CSPjqyylfFP9P777+u73/1u8nVzc7Mkqb6+Xi+99JKxwgDAWQTKKTfccIMSHvxNYQCA3HIvIgHA6+hQAABGfHWVVzY8OCni96EAAIygQwEA20yMvDzYoRAoAGAbgQIAMMLRQOEcCgDACDoUALDN0Q6FQAEA20xcNhyPm6nFIEZeAAAj6FAAwDYTIy8PdigECgDY5migMPICABhBhwIAtjnaoRAoAGCbo4HCyAsAYAQdCgDYZuI+lFjMTC0GESgAYJuJkZcHA4WRFwDY9lWgZLtk4J///Kd+9KMfadKkSSouLtbll1+u999/P/l+IpHQ6tWrNXnyZBUXF6uurk6ffPJJRscgUADAcf/+9781f/58nXvuuXrrrbf04Ycf6je/+Y3OP//85DZPPPGEnnnmGW3YsEG7du3ShAkTtGDBAg0PD6d9HEZeAGCb5ZHX448/rqqqKm3atCm5rqamJvnnRCKhdevW6cEHH9TixYslSa+88orKy8u1ZcsW3X777Wkdhw4FAGz76qR8NkthoSQpEomkLNFo9LTD/fGPf9ScOXP0/e9/X2VlZbryyiv1wgsvJN8/dOiQ+vv7VVdXl1wXCoU0b9487dy5M+0fi0ABgDxWVVWlUCiUXFpbW0/b5u9//7va29s1depUbd++Xffcc49+/vOf6+WXX5Yk9ff3S5LKy8tTPldeXp58Lx2MvADANhMjry8/Hw6HFQwGk6sDgcBpm8bjcc2ZM0ePPfaYJOnKK6/UgQMHtGHDBtXX12dXx/+gQwEA2wxe5RUMBlOWMwXK5MmTNX369JR1l156qT777DNJUkVFhSRpYGAgZZuBgYHke+kgUADAcfPnz9fBgwdT1n388ce68MILJZ06QV9RUaGurq7k+5FIRLt27VJtbW3ax2HkBQC2GRx5peO+++7Ttddeq8cee0w/+MEPtHv3bj3//PN6/vnnJUk+n09NTU165JFHNHXqVNXU1GjVqlWqrKzUkiVL0i8p058BAJAly4Eyd+5cdXZ2qqWlRQ8//LBqamq0bt06LVu2LLnN/fffr6GhId199906evSorrvuOm3btk1FRUXpl5TRDwAAyJ6JZ3l9edlwum699VbdeuutZ33f5/Pp4Ycf1sMPPzzqkjiHAgAwgg4FAGyzPPKyxXsVAYDrHA0URl4AACO8F3EA4DpHOxTvVQQArsvBVV42MPICABhBhwIAtjHyAgAY4WigMPICABjhvYgDANc52qF4ryIAcB2BAgAwgsuGAQA4OzoUALCNkRcAwAhHA4WRFwDACO9FHAC4ztEOxXsVAYDruMoLAICzo0MBANsYeQEAjHA0UBh5AQCM8F7EAYDrHO1QvFcRALiOQAEAGMFlwwAAnB0dCgDYxsgLAGCEo4HCyAsAYIT3Ig4AXOdoh+K9igDAdVzlBQDA2dGhAIBtjo68MupQWltbNXfuXJWUlKisrExLlizRwYMHx6o2AHDTV4GS7eIxGQVKd3e3Ghoa1Nvbq7ffflsnTpzQzTffrKGhobGqDwCQJzKKuG3btqW8fumll1RWVqa+vj5df/31RgsDAGc5OvLKqqLBwUFJUmlp6Vm3iUajikajydeRSCSbQwJA/nM0UEZ9lVc8HldTU5Pmz5+vGTNmnHW71tZWhUKh5FJVVTXaQwKAE+IqMLJ4zagramho0IEDB7R58+av3a6lpUWDg4PJJRwOj/aQAAAPG1XP1NjYqDfffFM9PT2aMmXK124bCAQUCARGVRwAuOjkyVNLtvvwmowCJZFIaMWKFers7NSOHTtUU1MzVnUBgLMIFJ0ac3V0dGjr1q0qKSlRf3+/JCkUCqm4uHhMCgQA5IeMAqW9vV2SdMMNN6Ss37Rpk+644w5TNQGA0+hQdGrkBQDIjquB4r3rzgAAecl7d8YAgONisew7jFjMTC0mESgAYBkjLwAAvgYdCgBY5mqHQqAAgGUECgDACFdPynMOBQBgBB0KAFjGyAsAYISrgcLICwBgBB0KAFjmaodCoACAZa4GCiMvAIARdCgAYJmr96EQKABgGSMvAAC+Bh0KAFjmaodCoACAZQQKAMAIV0/Kcw4FAMaZtWvXyufzqampKblueHhYDQ0NmjRpkiZOnKilS5dqYGAgo/0SKABg2Vcjr2yX0dizZ482btyomTNnpqy/77779MYbb+j1119Xd3e3Dh8+rNtuuy2jfRMoAGBZrgLl+PHjWrZsmV544QWdf/75yfWDg4N68cUX9eSTT+rGG2/U7NmztWnTJr333nvq7e1Ne/8ECgCMEw0NDbrllltUV1eXsr6vr08nTpxIWT9t2jRVV1dr586dae+fk/IAYJnJq7wikUjK+kAgoEAgcNr2mzdv1t69e7Vnz57T3uvv75ff79d5552Xsr68vFz9/f1p10SHAgCWmRx5VVVVKRQKJZfW1tbTjhcOh3Xvvffq1VdfVVFR0Zj9XHQoAJDHwuGwgsFg8vWZupO+vj4dOXJEV111VXJdLBZTT0+P1q9fr+3bt2tkZERHjx5N6VIGBgZUUVGRdi0ECgBYZvI+lGAwmBIoZ3LTTTdp//79KeuWL1+uadOm6YEHHlBVVZXOPfdcdXV1aenSpZKkgwcP6rPPPlNtbW3aNREoAGCZ7TvlS0pKNGPGjJR1EyZM0KRJk5Lr77rrLjU3N6u0tFTBYFArVqxQbW2trrnmmrSPQ6AAAPTUU0+poKBAS5cuVTQa1YIFC/Tcc89ltA8CBQAs88KzvHbs2JHyuqioSG1tbWpraxv1PgkUALDMC4EyFggUALCMh0MCAPA16FAAwDJGXgAAI1wNFEZeAAAj6FAAwDJXOxQCBQAsczVQGHkBAIygQwEAy1y9D4VAAQDLTp6UCguz34fXMPICABhBhwIAlrnaoRAoAGAZgQIAMMLVk/KcQwEAGEGHAgCWnTwpFWT533lGXgAAZwOFkRcAwAg6FACwzNUOhUABAMtcDRRGXgAAI+hQAMAyV+9DIVAAwLKTJyWfL/t9eA0jLwCAEXQoAGCZqx0KgQIAlhEoAAAjXA0UzqEAAIygQwEAy2Kx7DsULhsGABgZVzHyAgA4K6NAaW9v18yZMxUMBhUMBlVbW6u33nprrGoDACedPGlm8ZqMRl5TpkzR2rVrNXXqVCUSCb388stavHixPvjgA1122WVjVSMAOMXVkVdGgbJo0aKU148++qja29vV29tLoADAODfqk/KxWEyvv/66hoaGVFtbe9btotGootFo8nUkEhntIQHACSau0HLiKq/9+/ertrZWw8PDmjhxojo7OzV9+vSzbt/a2qqHHnooqyIBwCUnT0qJRHb78GKgZHyV1yWXXKJ9+/Zp165duueee1RfX68PP/zwrNu3tLRocHAwuYTD4awKBgB4U8Ydit/v18UXXyxJmj17tvbs2aOnn35aGzduPOP2gUBAgUAguyoBwCGudihZ39gYj8dTzpEAAL4egaJT46uFCxequrpax44dU0dHh3bs2KHt27ePVX0A4BwCRdKRI0f04x//WP/6178UCoU0c+ZMbd++Xd/73vfGqj4AQJ7IKFBefPHFsaoDAMaNWCz7DiUeN1OLSTwcEgAsO3lSKsjySYpeDBQeDgkAMIIOBQAsc7VDIVAAwDJXA4WRFwDACDoUALAsFsu+w8j2KrGxQKAAgGUnT2b/O+W9GCiMvAAARtChAIBlrnYoBAoAWEagAACMcDVQOIcCADCCDgUArIsrkcj2zkTv3dlIoACAdbEvl2z34S2MvAAARtChAIB1bnYoBAoAWOdmoDDyAgAYQYcCANbFlf1VWlzlBQBg5AUAwNnRoQCAdXFl32Ew8gIAMPICAJgRM7Skp7W1VXPnzlVJSYnKysq0ZMkSHTx4MGWb4eFhNTQ0aNKkSZo4caKWLl2qgYGBjH4qAgUAHNfd3a2Ghgb19vbq7bff1okTJ3TzzTdraGgouc19992nN954Q6+//rq6u7t1+PBh3XbbbRkdh5EXAFhn97Lhbdu2pbx+6aWXVFZWpr6+Pl1//fUaHBzUiy++qI6ODt14442SpE2bNunSSy9Vb2+vrrnmmrSOQ4cCANaZG3lFIpGUJRqNfuPRBwcHJUmlpaWSpL6+Pp04cUJ1dXXJbaZNm6bq6mrt3Lkz7Z+KQAGAPFZVVaVQKJRcWltbv3b7eDyupqYmzZ8/XzNmzJAk9ff3y+/367zzzkvZtry8XP39/WnXwsgLAKwzd5VXOBxWMBhMrg0EAl/7qYaGBh04cEDvvvtulsc/HYECANaZC5RgMJgSKF+nsbFRb775pnp6ejRlypTk+oqKCo2MjOjo0aMpXcrAwIAqKirSroiRFwA4LpFIqLGxUZ2dnXrnnXdUU1OT8v7s2bN17rnnqqurK7nu4MGD+uyzz1RbW5v2cehQAMA6u3fKNzQ0qKOjQ1u3blVJSUnyvEgoFFJxcbFCoZDuuusuNTc3q7S0VMFgUCtWrFBtbW3aV3hJBAoA5IDdy4bb29slSTfccEPK+k2bNumOO+6QJD311FMqKCjQ0qVLFY1GtWDBAj333HMZVUSgAIDjEonEN25TVFSktrY2tbW1jfo4BAoAWOfms7wIFACwjkABABjhZqBw2TAAwAg6FACwzs0OhUABAOsSyv6y4W++css2Rl4AACPoUADAOkZeAAAj3AwURl4AACPoUADAOjc7FAIFAKxzM1AYeQEAjKBDAQDr7D6+3hYCBQCsc3PkRaAAgHV2f2OjLZxDAQAYQYcCANYx8gIAGOHmSXlGXgAAI+hQAMA6Rl4AACPcDBRGXgAAI+hQAMA6NzsUAgUArOPGRgAAzooOBQCsc/M+FAIFAKzjHAoAwAg3AyWrcyhr166Vz+dTU1OToXIAAPlq1B3Knj17tHHjRs2cOdNkPQAwDtChJB0/flzLli3TCy+8oPPPP990TQDguJihxVtGFSgNDQ265ZZbVFdX943bRqNRRSKRlAUA4J6MR16bN2/W3r17tWfPnrS2b21t1UMPPZRxYQDgLjcvG86oQwmHw7r33nv16quvqqioKK3PtLS0aHBwMLmEw+FRFQoA7vjqTvlsFu8FSkYdSl9fn44cOaKrrroquS4Wi6mnp0fr169XNBpVYWFhymcCgYACgYCZagEAnpVRoNx0003av39/yrrly5dr2rRpeuCBB04LEwDAmbh5lVdGgVJSUqIZM2akrJswYYImTZp02noAwNm4GSg8HBIAYETWj17ZsWOHgTIAYDxxs0PhWV4AYB2BAgAwgvtQAAA4KzoUALDOzV8BTKAAgHVunkNh5AUAMIIOBQCsc7NDIVAAwDqu8gIA4KzoUADAOkZeAAAj3AwURl4AACPoUADAOjc7FAIFAKzjTnkAgBFcNgwAwFnRoQCAdZxDAQAY4WagMPICABhBhwIA1rnZoRAoAGCdm4HCyAsAYAQdCgBYx42NAAAjuLERAICzokMBAOtiyv7/8947KU+gAIB1BAoAwAg3A4VzKAAAI6x3KIlE4ss/jdg+dB46kesC8oT3rnbxomiuC/C4r76f//4bNZZGlP3f25MmCjHKeqAcO3bsyz/93vahgXFtba4LyBPHjh1TKBQak337/X5VVFSov///GdlfRUWF/H6/kX2Z4EvYieOkeDyuw4cPq6SkRD6fz+ahzyoSiaiqqkrhcFjBYDDX5XgS31F6+J7S48XvKZFI6NixY6qsrFRBwdidDRgeHtbIiJkJjd/vV1FRkZF9mWC9QykoKNCUKVNsHzYtwWDQM3+5vYrvKD18T+nx2vc0Vp3J/yoqKvJUCJjESXkAgBEECgDACAJFUiAQ0Jo1axQIBHJdimfxHaWH7yk9fE9usn5SHgDgJjoUAIARBAoAwAgCBQBgBIECADBi3AdKW1ubLrroIhUVFWnevHnavXt3rkvynJ6eHi1atEiVlZXy+XzasmVLrkvynNbWVs2dO1clJSUqKyvTkiVLdPDgwVyX5Tnt7e2aOXNm8obG2tpavfXWW7kuC4aM60B57bXX1NzcrDVr1mjv3r2aNWuWFixYoCNHjuS6NE8ZGhrSrFmz1NbWlutSPKu7u1sNDQ3q7e3V22+/rRMnTujmm2/W0NBQrkvzlClTpmjt2rXq6+vT+++/rxtvvFGLFy/WX//611yXBgPG9WXD8+bN09y5c7V+/XpJp54zVlVVpRUrVmjlypU5rs6bfD6fOjs7tWTJklyX4mmff/65ysrK1N3dreuvvz7X5XhaaWmpfv3rX+uuu+7KdSnI0rjtUEZGRtTX16e6urrkuoKCAtXV1Wnnzp05rAwuGBwclHTqH0ucWSwW0+bNmzU0NKTa2tpclwMDxu1vbPziiy8Ui8VUXl6esr68vFwfffRRjqqCC+LxuJqamjR//nzNmDEj1+V4zv79+1VbW6vh4WFNnDhRnZ2dmj59eq7LggHjNlCAsdLQ0KADBw7o3XffzXUpnnTJJZdo3759Ghwc1B/+8AfV19eru7ubUHHAuA2UCy64QIWFhRoYGEhZPzAwoIqKihxVhXzX2NioN998Uz09PZ79NQ255vf7dfHFF0uSZs+erT179ujpp5/Wxo0bc1wZsjVuz6H4/X7Nnj1bXV1dyXXxeFxdXV3Mc5GxRCKhxsZGdXZ26p133lFNTU2uS8ob8Xhc0Si/oNgF47ZDkaTm5mbV19drzpw5uvrqq7Vu3ToNDQ1p+fLluS7NU44fP65PP/00+frQoUPat2+fSktLVV1dncPKvKOhoUEdHR3aunWrSkpK1N/fL+nUL2wqLi7OcXXe0dLSooULF6q6ulrHjh1TR0eHduzYoe3bt+e6NJiQGOeeffbZRHV1dcLv9yeuvvrqRG9vb65L8pw///nPCUmnLfX19bkuzTPO9P1ISmzatCnXpXnKnXfembjwwgsTfr8/8e1vfztx0003Jf70pz/luiwYMq7vQwEAmDNuz6EAAMwiUAAARhAoAAAjCBQAgBEECgDACAIFAGAEgQIAMIJAAQAYQaAAAIwgUAAARhAoAAAjCBQAgBH/H9rTL1cBilqrAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:07.879180Z",
     "start_time": "2025-01-04T15:14:07.876948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "}plt.imshow(np.array(image))\n",
    "plt.title(text[0])"
   ],
   "id": "4d310e6ff984ed5a",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched '}' (1438563636.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[52], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    }plt.imshow(np.array(image))\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unmatched '}'\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:08.503072Z",
     "start_time": "2025-01-04T15:14:08.125133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "model.to(\"cuda\")"
   ],
   "id": "39441b32e74d97ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:17:02.469242Z",
     "start_time": "2025-01-04T15:17:02.462346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cpu\"\n",
    "model.to(device)"
   ],
   "id": "85c4a21b15ef5b37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:09.023140Z",
     "start_time": "2025-01-04T15:14:09.019426Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3050 4GB Laptop GPU', major=8, minor=6, total_memory=3801MB, multi_processor_count=20, uuid=87e3abde-eb07-aee8-4d8a-d5db8b976f6e, L2_cache_size=1MB)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54,
   "source": "torch. cuda. get_device_properties(0)",
   "id": "39054d37f54bc2e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:09.369238Z",
     "start_time": "2025-01-04T15:14:09.365875Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 55,
   "source": "from src.lib.dataset.ROCODataset import ImageTextDataset",
   "id": "6ab67def34153508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:09.738357Z",
     "start_time": "2025-01-04T15:14:09.732848Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'test_json': '/home/felipe/Projects/roco-image-captioning/roco-dataset/json/test_dataset.json',\n",
      "             'train_json': '/home/felipe/Projects/roco-image-captioning/roco-dataset/json/train_dataset.json',\n",
      "             'validation_json': '/home/felipe/Projects/roco-image-captioning/roco-dataset/json/validation_dataset.json'},\n",
      " 'input_size': [224, 224],\n",
      " 'optimizer': {'lr': 0.0001,\n",
      "               'lr_scheduler_rate': 0.1,\n",
      "               'method': 'adam',\n",
      "               'weight_decay': 0.0},\n",
      " 'registry': {'checkpoints_dir': 'outputs/model_checkpoints',\n",
      "              'name': 'roco-clip',\n",
      "              'project': 'roco-debug',\n",
      "              'wandb_root_dir': 'outputs/wandb_outputs'},\n",
      " 'train': {'batch_size': 4,\n",
      "           'early_stopping_patience': 5,\n",
      "           'gradient_clip_val': 100.0,\n",
      "           'limit_train_batches': 0.1,\n",
      "           'log_every_n_steps': 4,\n",
      "           'max_epochs': 10,\n",
      "           'max_seq_length': 70}}\n"
     ]
    }
   ],
   "execution_count": 56,
   "source": [
    "import yaml\n",
    "cfg = yaml.safe_load(open(\"/home/felipe/Projects/roco-image-captioning/config.yaml\"))\n",
    "from pprint import pprint\n",
    "pprint(cfg)"
   ],
   "id": "8b00847079267130"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:11.197138Z",
     "start_time": "2025-01-04T15:14:11.031061Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 57,
   "source": [
    "train_dataset = ImageTextDataset(\n",
    "    name=\"train\",\n",
    "    cfg=cfg,\n",
    "    root=\"/home/felipe/Projects/roco-image-captioning\",\n",
    ")\n",
    "\n",
    "val_dataset = ImageTextDataset(\n",
    "    name=\"val\",\n",
    "    cfg=cfg,\n",
    "    root=\"/home/felipe/Projects/roco-image-captioning\",\n",
    ")"
   ],
   "id": "a95aca3d367dfa92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:14.001319Z",
     "start_time": "2025-01-04T15:14:13.997560Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65391, 8169)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58,
   "source": "len(train_dataset), len(val_dataset)",
   "id": "528d976b9260a627"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:15.003162Z",
     "start_time": "2025-01-04T15:14:15.000242Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 59,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ],
   "id": "1a4438a579829010"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:16.647472Z",
     "start_time": "2025-01-04T15:14:16.643492Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 60,
   "source": [
    "batch_size = 4\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "f0b3c7f57eae4496"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:14:18.960933Z",
     "start_time": "2025-01-04T15:14:18.958363Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 61,
   "source": "import torch",
   "id": "d854316d9e6d68a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:17:21.882766Z",
     "start_time": "2025-01-04T15:17:21.877768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text_and_images(images, targets):\n",
    "    inputs = processor(text=targets, images=images, return_tensors=\"pt\", padding=True)\n",
    "    return inputs    \n",
    "    \n",
    "def create_pairs(images, targets):\n",
    "    size = images.shape[0]\n",
    "    print(type(images), type(targets), images.shape, len(targets), targets)\n",
    "    \n",
    "    \n",
    "    positive_pairs = process_text_and_images(images, targets)\n",
    "    \n",
    "    negative_pairs = []\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if i == j:\n",
    "                continue\n",
    "            pass    \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    processed = process_text_and_images(images, targets)\n",
    "    \n",
    "    print(processed[\"pixel_values\"].shape, processed[\"input_ids\"].shape)\n",
    "    \n",
    "    pairs = {\"input_ids\": [], \"attention_mask\": [], \"pixel_values\": []}\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            \n",
    "            # build every possible image / text pair\n",
    "            pairs[\"input_ids\"].append(processed[\"input_ids\"][i])  \n",
    "            pairs[\"attention_mask\"].append(processed[\"attention_mask\"][i])  # masks padding added to text\n",
    "            pairs[\"pixel_values\"].append(processed[\"pixel_values\"][j])\n",
    "           \n",
    "            # labels for contrastive learning            \n",
    "            if i == j:  # positive sample\n",
    "                labels.append(1)\n",
    "            else:  # negative sample\n",
    "                labels.append(0)\n",
    "    \n",
    "    # convert to tensor\n",
    "    for k, v in pairs.items():\n",
    "        pairs[k] = torch.stack(v, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return pairs, labels"
   ],
   "id": "f82333b1b147e422",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:29:03.107157Z",
     "start_time": "2025-01-04T15:29:02.606123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "for batch in train_dl:\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    inputs = processor(text=targets, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    logits_per_text = outputs.logits_per_text\n",
    "    \n",
    "    print(logits_per_image.shape, logits_per_text.shape)\n",
    "    print(logits_per_text.softmax(dim=1))\n",
    "    print(logits_per_text.softmax(dim=1).argmax(dim=1))\n",
    "    print(torch.nn.functional.one_hot(logits_per_text.softmax(dim=1).argmax(dim=1), num_classes=logits_per_text.shape[-1]))\n",
    "    \n",
    "    print(targets)\n",
    "    break\n",
    "\n",
    "    # pairs, labels = create_pairs(image, target)\n",
    "    # \n",
    "    # break\n",
    "    # for k in pairs:\n",
    "    #     pairs[k] = pairs[k].to(device)    \n",
    "    # \n",
    "    # text_features = model.get_text_features(input_ids=pairs[\"input_ids\"],\n",
    "    #                                         attention_mask=pairs[\"attention_mask\"],)\n",
    "    # image_features = model.get_image_features(pixel_values=pairs[\"pixel_values\"],)    \n"
   ],
   "id": "3b530ec324bb101b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([4, 4])\n",
      "tensor([[0.3465, 0.1196, 0.2388, 0.2952],\n",
      "        [0.3178, 0.1364, 0.2258, 0.3200],\n",
      "        [0.2980, 0.1723, 0.2513, 0.2785],\n",
      "        [0.2468, 0.2133, 0.2464, 0.2935]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 3, 0, 3])\n",
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1]])\n",
      "(' Left ventriculography. Right anterior oblique, 30° view of grade 3 to', \" Bowen's Disease, OCT aspect. E: epidermis, which in the lesional area\", ' Normal left anterior descending (LAD) and right coronary (RCA) arteri', ' Post-reduction radiograph of the wrist. Mild shortening of radial len')\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:42:12.629237Z",
     "start_time": "2025-01-03T09:42:12.621892Z"
    }
   },
   "cell_type": "code",
   "source": "|import einops",
   "id": "8bdb99894f740769",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:42:13.119397Z",
     "start_time": "2025-01-03T09:42:13.115773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scaled_pairwise_cosine_sim_loss(text_features, image_features, labels, weight=0.5):\n",
    "    text_logits = torch.bmm(einops.rearrange(text_features, \"b d -> b 1 d\"),\n",
    "                            einops.rearrange(image_features, \"b d -> b d 1\"))\n",
    "    \n",
    "    image_logits = torch.bmm(einops.rearrange(image_features, \"b d -> b 1 d\"),\n",
    "                             einops.rearrange(image_features, \"b d -> b d 1\"))\n",
    "    \n",
    "    text_logits = einops.rearrange(text_logits, \"b 1 1 -> b\")\n",
    "    image_logits = einops.rearrange(image_logits, \"b 1 1 -> b\")\n",
    "    \n",
    "    normalization = torch.linalg.vector_norm(text_features, dim=1) * torch.linalg.norm(image_features, dim=1)\n",
    "    \n",
    "    text_logits /= normalization\n",
    "    image_logits /= normalization\n",
    "    \n",
    "    text_logits = torch.log(text_logits)\n",
    "    image_logits = torch.log(image_logits)\n",
    "\n",
    "    text_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        text_logits, labels\n",
    "    )\n",
    "    \n",
    "    image_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        image_logits, labels\n",
    "    )\n",
    "    \n",
    "    return weight * text_loss + (1 - weight) * image_loss\n",
    "            "
   ],
   "id": "11453063a071a70e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:42:13.529651Z",
     "start_time": "2025-01-03T09:42:13.525752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = scaled_pairwise_cosine_sim_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "lr_schedule = torch.optim.lr_scheduler.LinearLR(optimizer)"
   ],
   "id": "ee39b0b8fda50397",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:43:49.601245Z",
     "start_time": "2025-01-03T09:42:13.840945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "pbar = tqdm(total=len(train_dataset), desc=\"Training\")\n",
    "for batch in train_dl:\n",
    "    image = batch[0]\n",
    "    target = batch[1]\n",
    "    pairs, labels = create_pairs(image, target)\n",
    "    for k in pairs:\n",
    "        pairs[k] = pairs[k].to(device)    \n",
    "    \n",
    "    text_features = model.get_text_features(input_ids=pairs[\"input_ids\"],\n",
    "                                            attention_mask=pairs[\"attention_mask\"],)\n",
    "    image_features = model.get_image_features(pixel_values=pairs[\"pixel_values\"],)    \n",
    "\n",
    "    loss = loss_fn(text_features, image_features, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_schedule.step()\n",
    "\n",
    "    pbar.set_description(f\"Training: loss = {loss.item()}\")\n",
    "    pbar.update()"
   ],
   "id": "1fbbe5dc0559d035",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/65397 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Training: loss = 0.5623372793197632:   0%|          | 34/65397 [01:35<39:26:46,  2.17s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      2\u001B[0m pbar \u001B[38;5;241m=\u001B[39m tqdm(total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_dataset), desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/src/lib/dataset/ROCODataset.py:87\u001B[0m, in \u001B[0;36mImageTextDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     84\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_image(index)\n\u001B[1;32m     85\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_target(index)\n\u001B[0;32m---> 87\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image, target\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:973\u001B[0m, in \u001B[0;36mRandomResizedCrop.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    965\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    966\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    967\u001B[0m \u001B[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped and resized.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    970\u001B[0m \u001B[38;5;124;03m    PIL Image or Tensor: Randomly cropped and resized image.\u001B[39;00m\n\u001B[1;32m    971\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    972\u001B[0m i, j, h, w \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_params(img, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mratio)\n\u001B[0;32m--> 973\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresized_crop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mantialias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:650\u001B[0m, in \u001B[0;36mresized_crop\u001B[0;34m(img, top, left, height, width, size, interpolation, antialias)\u001B[0m\n\u001B[1;32m    648\u001B[0m     _log_api_usage_once(resized_crop)\n\u001B[1;32m    649\u001B[0m img \u001B[38;5;241m=\u001B[39m crop(img, top, left, height, width)\n\u001B[0;32m--> 650\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mantialias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[1;32m    475\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    476\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[0;32m--> 477\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpil_interpolation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39minterpolation\u001B[38;5;241m.\u001B[39mvalue, antialias\u001B[38;5;241m=\u001B[39mantialias)\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:250\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot inappropriate size arg: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/PIL/Image.py:2365\u001B[0m, in \u001B[0;36mImage.resize\u001B[0;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[1;32m   2353\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2354\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce(factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[1;32m   2355\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduce)\n\u001B[1;32m   2356\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[1;32m   2357\u001B[0m         )\n\u001B[1;32m   2358\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2359\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[1;32m   2360\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[1;32m   2361\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[1;32m   2362\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[1;32m   2363\u001B[0m         )\n\u001B[0;32m-> 2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbox\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:47:25.957227Z",
     "start_time": "2025-01-03T09:47:15.296644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shaps = set()\n",
    "for batch in train_dl:\n",
    "    image = batch[0]\n",
    "    shaps.add(image.shape)"
   ],
   "id": "3761775ce13c3afd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported JPEG process: SOF type 0xc6",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m shaps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshaps\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/src/lib/dataset/ROCODataset.py:84\u001B[0m, in \u001B[0;36mImageTextDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m---> 84\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_target(index)\n\u001B[1;32m     87\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(image)\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/src/lib/dataset/ROCODataset.py:76\u001B[0m, in \u001B[0;36mImageTextDataset._load_image\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_image\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx: \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m     75\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_paths[idx]\n\u001B[0;32m---> 76\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mread_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mImageReadMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRGB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;66;03m# image = Image.open(path)\u001B[39;00m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m image\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torchvision/io/image.py:331\u001B[0m, in \u001B[0;36mread_image\u001B[0;34m(path, mode, apply_exif_orientation)\u001B[0m\n\u001B[1;32m    329\u001B[0m     _log_api_usage_once(read_image)\n\u001B[1;32m    330\u001B[0m data \u001B[38;5;241m=\u001B[39m read_file(path)\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdecode_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapply_exif_orientation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapply_exif_orientation\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torchvision/io/image.py:318\u001B[0m, in \u001B[0;36mdecode_image\u001B[0;34m(input, mode, apply_exif_orientation)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mode, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    317\u001B[0m     mode \u001B[38;5;241m=\u001B[39m ImageReadMode[mode\u001B[38;5;241m.\u001B[39mupper()]\n\u001B[0;32m--> 318\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode_image\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapply_exif_orientation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/.venv/lib/python3.12/site-packages/torch/_ops.py:1116\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_torchbind_op_overload \u001B[38;5;129;01mand\u001B[39;00m _must_dispatch_in_python(args, kwargs):\n\u001B[1;32m   1115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _call_overload_packet_from_python(\u001B[38;5;28mself\u001B[39m, args, kwargs)\n\u001B[0;32m-> 1116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Unsupported JPEG process: SOF type 0xc6"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T09:45:32.329063Z",
     "start_time": "2025-01-03T09:45:32.312252Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b3ef5e85817151ff",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'set' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m A \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mA\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m      3\u001B[0m A\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +=: 'set' and 'int'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "811fad93dc61f56b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T11:31:35.094861Z",
     "start_time": "2025-01-03T11:31:34.866143Z"
    }
   },
   "cell_type": "code",
   "source": "from src.lib.dataset.CLIP_ROCODataset import CLIPDataset",
   "id": "ca410a892965d59d",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ROCODataset'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCLIP_ROCODataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CLIPDataset\n",
      "File \u001B[0;32m~/Projects/roco-image-captioning/src/lib/dataset/CLIP_ROCODataset.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mROCODataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageTextDataset\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CLIPProcessor\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'ROCODataset'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = ImageTextDataset(\n",
    "    name=\"train\",\n",
    "    cfg=cfg,\n",
    "    root=\"/home/felipe/Projects/roco-image-captioning\",\n",
    ")\n",
    "\n",
    "val_dataset = ImageTextDataset(\n",
    "    name=\"val\",\n",
    "    cfg=cfg,\n",
    "    root=\"/home/felipe/Projects/roco-image-captioning\",\n",
    ")"
   ],
   "id": "83fb2468cdf16607"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
